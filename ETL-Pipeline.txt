================================================================================
SEC FILING ETL PIPELINE - COMPLETE DOCUMENTATION
================================================================================

Project: Financial Data Aggregator
Component: SEC Filing Text Extraction and Analysis Pipeline
Author: System
Date: 2025-12-11

================================================================================
OVERVIEW
================================================================================

This ETL pipeline demonstrates a production-ready approach to extracting,
transforming, and loading SEC filing data with full text analysis capabilities.

The pipeline showcases:
✓ API integration with SEC EDGAR
✓ Unstructured text extraction and parsing
✓ NLP-based section extraction and financial metrics detection
✓ Star schema data warehouse design
✓ Scalable batch processing with error handling
✓ Ready for downstream ML/RAG applications

================================================================================
ARCHITECTURE
================================================================================

Pipeline Flow:
┌─────────────┐     ┌──────────────┐     ┌─────────────┐
│   EXTRACT   │────▶│  TRANSFORM   │────▶│    LOAD     │
└─────────────┘     └──────────────┘     └─────────────┘
      │                    │                     │
  SEC EDGAR API      Text Analysis       Star Schema DB
  Filing URLs        Section Parsing     (PostgreSQL/SQLite)
  Metadata          Financial Metrics
  Full Text         Risk Keywords

Data Model (Star Schema):
┌─────────────────┐
│  dim_company    │◀───┐
└─────────────────┘    │
┌─────────────────┐    │    ┌──────────────────┐
│   dim_date      │◀───┼────│ fact_sec_filing  │
└─────────────────┘    │    └──────────────────┘
┌─────────────────┐    │              │
│ dim_filing_type │◀───┘              │
└─────────────────┘                   │
┌─────────────────┐                   │    ┌──────────────────────┐
│ dim_data_source │◀──────────────────┼────│ fact_filing_analysis │
└─────────────────┘                   │    └──────────────────────┘
                                      ▼
                              Stores: Filing text,
                              Metadata, URLs

                                      ▼
                              Stores: Extracted sections,
                              Financial mentions,
                              Risk keywords,
                              Word counts

================================================================================
PHASE 1: EXTRACT
================================================================================

Purpose: Fetch SEC filing metadata and full text from SEC EDGAR API

Components:
-----------
1. SECEdgarExtractor (src/extractors/sec_edgar.py)
   - Fetches filing metadata from SEC EDGAR API
   - Resolves ticker → CIK mapping
   - Rate-limited to comply with SEC fair access policy (10 req/sec)
   
2. Text Extraction
   - Downloads full HTML filing documents
   - Extracts clean text content
   - Strips scripts, styles, and formatting

Input:
------
- Tickers: ['AAPL', 'MSFT', 'GOOGL']
- Filing types: ['10-K', '10-Q']
- Date range: Last 12 months
- Count: 3 filings per ticker

Output:
-------
DataFrame with columns:
- ticker: Stock symbol
- cik: SEC CIK identifier
- filing_type: 10-K, 10-Q, etc.
- filing_date: Date of filing
- accession_number: Unique SEC identifier
- filing_url: URL to filing on SEC.gov
- filing_text: Full extracted text (100K-500K chars)
- filing_size: Size in bytes

Example:
--------
ticker | filing_type | filing_date | filing_text_length
-------|-------------|-------------|-------------------
AAPL   | 10-K        | 2024-10-31  | 342,156 chars
AAPL   | 10-Q        | 2024-08-01  | 156,892 chars
MSFT   | 10-K        | 2024-07-30  | 298,443 chars

Data Quality Checks:
-------------------
✓ CIK resolution succeeds
✓ Filing URL is accessible
✓ Text extraction returns non-empty content
✓ No duplicates by accession_number

================================================================================
PHASE 2: TRANSFORM
================================================================================

Purpose: Analyze filing text to extract structured insights

Components:
-----------
1. FilingAnalyzer (src/analyzers/filing_analyzer.py)
   
   Methods:
   --------
   a) extract_section(text, section_key)
      - Uses regex patterns to identify Item sections
      - Patterns: "Item 1. Business", "Item 1A. Risk Factors", etc.
      - Returns section text or None
   
   b) extract_all_sections(text)
      - Extracts all standard 10-K/10-Q sections:
        * Business (Item 1)
        * Risk Factors (Item 1A)
        * MD&A (Item 7)
        * Financial Statements (Item 8)
        * Controls (Item 9A)
   
   c) extract_financial_mentions(text)
      - Regex patterns for financial metrics:
        * Revenue: "revenue of $394 billion"
        * Net Income: "net income of $100 billion"
        * EPS: "earnings per share of $6.43"
        * Cash: "cash and cash equivalents of $30 billion"
        * Debt: "total debt of $120 billion"
      - Returns dict of metric → list of mentions
   
   d) extract_risk_keywords(risk_factors_text)
      - Searches for risk-related terms:
        uncertainty, volatile, regulatory, cybersecurity,
        pandemic, supply chain, inflation, etc.
      - Counts frequency of each keyword
      - Returns sorted list by frequency
   
   e) calculate_section_stats(section_text)
      - Word count
      - Character count
      - Sentence count
      - Average word/sentence length

2. Date Dimension Transformation
   - Converts filing dates to date dimension
   - Adds: year, quarter, month, day_of_week, is_weekend

Analysis Output:
----------------
For each filing:
{
  'ticker': 'AAPL',
  'filing_type': '10-K',
  'filing_date': '2024-10-31',
  'sections': {
    'business': {
      'stats': {'word_count': 12543, 'char_count': 78234, ...},
      'preview': 'Apple Inc. designs, manufactures...'
    },
    'risk_factors': {...},
    'mda': {...}
  },
  'financial_mentions': {
    'revenue': ['$394 billion', '$383 billion'],
    'net_income': ['$100 billion'],
    'earnings': ['$6.43']
  },
  'risk_keywords': [
    {'keyword': 'regulatory', 'count': 45},
    {'keyword': 'competition', 'count': 32}
  ],
  'metadata': {
    'sections_found': 5,
    'total_word_count': 145230,
    'total_mentions': 12
  }
}

Transformation Logic:
--------------------
1. For each filing with text:
   a) Extract all sections using regex patterns
   b) Calculate statistics for each section
   c) Parse MD&A for financial mentions
   d) Extract risk keywords from Risk Factors
   e) Aggregate metadata

2. Data Quality:
   ✓ Validate section extraction (min 3 sections)
   ✓ Check financial mention format (contains $)
   ✓ Verify word counts are reasonable (> 10K words for 10-K)

================================================================================
PHASE 3: LOAD
================================================================================

Purpose: Store filings and analysis results in star schema database

Components:
-----------
1. SECFilingLoader (src/loaders/sec_loader.py)
   - Loads filing metadata and full text
   - Upserts to fact_sec_filing table
   - Links to dimension tables via foreign keys

2. FilingAnalysisLoader (src/loaders/filing_analysis_loader.py)
   - Stores analysis results
   - Serializes JSON arrays (mentions, keywords)
   - Links to filing via filing_id

Database Schema:
----------------

Dimension Tables:
┌──────────────────┐
│  dim_company     │
├──────────────────┤
│ company_id (PK)  │
│ ticker           │
│ company_name     │
│ sector           │
│ industry         │
└──────────────────┘

┌──────────────────┐
│  dim_date        │
├──────────────────┤
│ date_id (PK)     │
│ date             │
│ year, quarter    │
│ month, week, day │
└──────────────────┘

┌──────────────────┐
│ dim_filing_type  │
├──────────────────┤
│ filing_type_id   │
│ filing_type      │ (10-K, 10-Q, 8-K)
│ description      │
│ category         │ (Annual, Quarterly)
└──────────────────┘

Fact Tables:
┌────────────────────────┐
│  fact_sec_filing       │
├────────────────────────┤
│ filing_id (PK)         │
│ company_id (FK)        │
│ filing_type_id (FK)    │
│ date_id (FK)           │
│ source_id (FK)         │
│ cik                    │
│ accession_number       │ (Unique)
│ filing_url             │
│ filing_text            │ (Full text, TEXT)
│ filing_size            │
│ created_at             │
└────────────────────────┘

┌─────────────────────────────┐
│  fact_filing_analysis       │
├─────────────────────────────┤
│ analysis_id (PK)            │
│ filing_id (FK, Unique)      │
│ company_id (FK)             │
│ date_id (FK)                │
│ sections_found              │
│ business_word_count         │
│ risk_factors_word_count     │
│ mda_word_count              │
│ financials_word_count       │
│ revenue_mentions (JSON)     │
│ net_income_mentions (JSON)  │
│ earnings_mentions (JSON)    │
│ cash_mentions (JSON)        │
│ debt_mentions (JSON)        │
│ risk_keywords (JSON)        │
│ total_word_count            │
│ financial_mentions_count    │
│ analyzed_at                 │
└─────────────────────────────┘

Loading Process:
----------------
1. Load or update dimension tables:
   - dim_company (upsert by ticker)
   - dim_date (upsert by date)
   - dim_filing_type (insert standard types)

2. Load fact_sec_filing:
   - Batch insert with upsert on accession_number
   - Store full filing text in filing_text column
   - Link to dimensions via foreign keys

3. Load fact_filing_analysis:
   - For each filing, store analysis results
   - Serialize arrays as JSON strings
   - Link to filing via filing_id (one-to-one)

4. Data Quality Checks:
   ✓ All foreign keys resolve
   ✓ No duplicate accession_numbers
   ✓ Analysis exists for filings with text
   ✓ JSON serialization succeeds

Performance:
-----------
- Batch size: 50 records per commit
- Transaction per batch (rollback on error)
- Indexes on: ticker, date, filing_type, accession_number
- Average load time: ~30 seconds for 10 filings with analysis

================================================================================
USAGE EXAMPLES
================================================================================

Basic Usage:
------------
cd /home/archy/Desktop/Server/FinancialData/financial_data_aggregator
source venv/bin/activate

# Run complete ETL pipeline with analysis
python sec_etl_pipeline.py --tickers AAPL MSFT --count 3

# Without analysis (text extraction only)
python sec_etl_pipeline.py --tickers AAPL --count 5 --no-analyze

# Specific date range
python sec_etl_pipeline.py \
    --tickers AAPL MSFT GOOGL \
    --start-date 2024-01-01 \
    --end-date 2024-12-31 \
    --count 10

Expected Output:
---------------
PHASE 1: EXTRACT
  ✓ Extracted 9 filing metadata records
  ✓ Extracted text from 9/9 filings (avg: 250K chars)

PHASE 2: TRANSFORM
  ✓ Analyzed 9 filings
  ✓ Found avg 5.2 sections per filing
  ✓ Extracted avg 8 financial mentions per filing

PHASE 3: LOAD
  ✓ Loaded 3 companies
  ✓ Loaded 9 dates
  ✓ Loaded 9 filing records with text
  ✓ Loaded 9 analysis records

PIPELINE COMPLETED: 9 filings processed in 145 seconds

================================================================================
QUERYING THE DATA
================================================================================

Example SQL Queries:
-------------------

1. Get all 10-K filings with analysis:
   SELECT 
       c.ticker,
       c.company_name,
       d.date as filing_date,
       a.sections_found,
       a.total_word_count,
       a.financial_mentions_count
   FROM fact_filing_analysis a
   JOIN fact_sec_filing f ON a.filing_id = f.filing_id
   JOIN dim_company c ON f.company_id = c.company_id
   JOIN dim_date d ON f.date_id = d.date_id
   JOIN dim_filing_type ft ON f.filing_type_id = ft.filing_type_id
   WHERE ft.filing_type = '10-K'
   ORDER BY d.date DESC;

2. Compare risk keywords across companies:
   SELECT 
       c.ticker,
       a.risk_keywords
   FROM fact_filing_analysis a
   JOIN dim_company c ON a.company_id = c.company_id
   WHERE a.risk_keywords IS NOT NULL;

3. Track MD&A length over time:
   SELECT 
       c.ticker,
       d.year,
       d.quarter,
       AVG(a.mda_word_count) as avg_mda_words
   FROM fact_filing_analysis a
   JOIN dim_company c ON a.company_id = c.company_id
   JOIN dim_date d ON a.date_id = d.date_id
   GROUP BY c.ticker, d.year, d.quarter
   ORDER BY c.ticker, d.year, d.quarter;

================================================================================
DOWNSTREAM APPLICATIONS
================================================================================

1. RAG (Retrieval-Augmented Generation) System:
   - Chunk filing_text by sections
   - Generate embeddings for each chunk
   - Store in vector database with metadata filters
   - Query: "What are Apple's main risk factors?"

2. Sentiment Analysis:
   - Analyze risk_factors section sentiment over time
   - Track MD&A tone changes
   - Compare sentiment across companies

3. Financial Metrics Time Series:
   - Extract revenue_mentions for each quarter
   - Build time series of financial metrics
   - Compare to actual stock prices

4. Knowledge Graph:
   - Extract entities (companies, products, locations)
   - Build relationships from filing text
   - Track strategic changes over time

================================================================================
TECHNICAL SPECIFICATIONS
================================================================================

Technology Stack:
----------------
- Language: Python 3.9+
- Database: PostgreSQL / SQLite
- ORM: SQLAlchemy
- API: SEC EDGAR (public, no key required)
- Text Processing: regex, BeautifulSoup
- Logging: loguru

Dependencies:
------------
- sqlalchemy
- pandas
- requests
- beautifulsoup4
- lxml
- loguru

Performance Metrics:
-------------------
- Extraction rate: ~20 seconds per filing (with rate limiting)
- Analysis rate: ~5 seconds per filing
- Database load: ~1 second per filing
- Total pipeline: ~3-4 minutes per ticker (3 filings)

Error Handling:
--------------
✓ Rate limit compliance (SEC: 10 req/sec)
✓ Network timeout handling
✓ Text extraction fallback
✓ Transaction rollback on load errors
✓ Comprehensive logging at all stages

================================================================================
FILES CREATED
================================================================================

New Components:
--------------
1. src/analyzers/filing_analyzer.py
   - FilingAnalyzer class
   - Section extraction methods
   - Financial metrics parsing
   - Risk keyword detection

2. src/loaders/filing_analysis_loader.py
   - FilingAnalysisLoader class
   - Analysis result storage
   - JSON serialization

3. src/models/facts.py (updated)
   - FactFilingAnalysis model
   - New fact table for analysis results

4. sec_etl_pipeline.py
   - Complete ETL pipeline
   - Text extraction integration
   - Analysis orchestration

5. ETL-Pipeline.txt
   - This documentation

================================================================================
NEXT STEPS FOR JOB APPLICATION
================================================================================

To Demonstrate:
--------------
✅ Complete ETL pipeline (Extract → Transform → Load)
✅ Unstructured text processing
✅ Star schema dimensional modeling
✅ Production-ready error handling
✅ Scalable batch processing
✅ Data quality checks

To Add (Optional):
------------------
□ Jupyter notebook with demo queries
□ Simple RAG implementation
□ Sentiment analysis visualization
□ Architecture diagram
□ Performance benchmarks

Key Talking Points:
------------------
1. "I built a production ETL pipeline that extracts and analyzes SEC filings"
2. "Processes unstructured text (100K+ chars) into structured metrics"
3. "Star schema design enables efficient analytics queries"
4. "Text extraction ready for downstream ML/RAG applications"
5. "Handles both structured (prices) and unstructured (filings) data"

================================================================================
END OF DOCUMENTATION
================================================================================
